# LiteLLM config.yaml
# this one on audiollm adress

model_list:
  # 1) Local vLLM served model (OpenAI-compatible)
  - model_name: glm_local
    litellm_params:
      model: openai/GLM_air_fp8
      api_base: http://nid-sc-34:16667/v1
      api_key: "EMPTY"
      supports_reasoning: True
      request_timeout: 540

#  retired due parsing issues    
#  - model_name: glm_local_2
#    litellm_params:
#      model: openai/"models/GLM-4.5-Air-UD-Q4_K_XL-00001-of-00002.gguf
#      api_base: http://nid-sc-27:8124/v1
#      api_key: "EMPTY"
#      supports_reasoning: True
#      request_timeout: 540

  - model_name: glm_cloud
    litellm_params:
      model: openai/z-ai/glm-4.6
      api_base: https://nano-gpt.com/api/v1
      api_key: ${NANOGPT_API_KEY}
      request_timeout: 540
      
  - model_name: glm_cloud_thinking
    litellm_params:
      model: openai/z-ai/glm-4.6:thinking
      api_base: https://nano-gpt.com/api/v1
      api_key: ${NANOGPT_API_KEY}
      supports_reasoning: True
      request_timeout: 540

#  - model_name: kimi_cloud_thinking
#    litellm_params:
#      model: openai/moonshotai/kimi-k2-thinking
#      api_base: https://nano-gpt.com/api/v1
#      api_key: ${NANOGPT_API_KEY}
#      supports_reasoning: True
#      request_timeout: 10

# Router / reliability settings
router_settings:
  # Try the active model up to 2 times
  num_retries: 2
  fallbacks:
    - {"glm_cloud": ["glm_local"]}
    - {"glm_cloud_thinking": ["glm_local"]}

# (Optional) proxy server bind â€” change to 0.0.0.0 only if you intend remote access
# THESE HAS TO BE SET USING --host --port, no support for startup for now (TODO)
general_settings:
  server:
    host: 0.0.0.0
    port: 17771
  # Enable/disable experimental OpenAI Responses API support
  enable_responses_endpoint: false
