# LiteLLM config.yaml
# this one on audiollm adress

model_list:
  - model_name: GLM-4.6-Thinking
    litellm_params:
      api_type: openai
      model: z-ai/glm-4.6:thinking
      api_base: https://nano-gpt.com/api/v1thinking
      api_key: ${NANOGPT_API_KEY}
      supports_reasoning: True
      request_timeout: 540

  - model_name: MiniMax-M2
    litellm_params:
      api_type: openai
      model: MiniMax-M2
      api_base: https://api.minimax.io/v1
      api_key: ${MINIMAX_API_KEY}
      request_timeout: 540
    
  - model_name: MiniMax-M2-2
    litellm_params:
      api_type: openai
      model: MiniMax-M2
      api_base: https://nano-gpt.com/api/v1thinking
      api_key: ${NANOGPT_API_KEY}
      supports_reasoning: True
      request_timeout: 540

  - model_name: Kimi-K2-Thinking
    litellm_params:
      api_type: openai
      model: moonshotai/kimi-k2-thinking
      api_base: https://nano-gpt.com/api/v1thinking
      api_key: ${NANOGPT_API_KEY}
      supports_reasoning: True
      request_timeout: 540

# Router / reliability settings
router_settings:
  # Try the active model up to 2 times
  num_retries: 2

# (Optional) proxy server bind â€” change to 0.0.0.0 only if you intend remote access
# THESE HAS TO BE SET USING --host --port, no support for startup for now (TODO)
general_settings:
  server:
    host: 0.0.0.0
    port: 6666
  # Enable/disable experimental OpenAI Responses API support
  enable_responses_endpoint: false
